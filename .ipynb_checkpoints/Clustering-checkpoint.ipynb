{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('dataset_sentimens.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_variables = ['public_metrics.retweet_count', \n",
    "                    'public_metrics.reply_count', \n",
    "                    'public_metrics.quote_count', \n",
    "                    #'author_id', \n",
    "                    #'is_retweet', \n",
    "                    #'neun_euro_context', \n",
    "                    'positive', \n",
    "                    'negative', \n",
    "                    'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_variables = ['public_metrics.retweet_count', \n",
    "                    'public_metrics.reply_count', \n",
    "                    'public_metrics.quote_count', \n",
    "                    #'author_id', \n",
    "                    #'is_retweet', \n",
    "                    #'neun_euro_context', \n",
    "                    'positive', \n",
    "                    'negative', \n",
    "                    'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_german = list()\n",
    "with open('stopp_w√∂rter_deutsch.txt','r') as file:\n",
    "   \n",
    "    # reading each line    \n",
    "    for line in file:\n",
    "   \n",
    "        # reading each word        \n",
    "        for word in line.split(','):\n",
    "   \n",
    "            # displaying the words           \n",
    "            stop_words_german.append(word.replace(' ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[cluster_variables].to_numpy()\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6740"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egon/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['wahr'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].str.replace('\\n',' ')\n",
    "corpus = df['text']\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words_german,\n",
    "                            strip_accents='unicode',\n",
    "#                             max_df=0.8,\n",
    "#                             min_df=100\n",
    "                            )\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "#X = np.hstack((df[['positive', 'negative', 'neutral']].to_numpy(),X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6748, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['positive', 'negative', 'neutral']].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[['positive', 'negative', 'neutral']].to_numpy()\n",
    "tmp2 = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((tmp,tmp2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51704931, -0.63552046,  2.95639515, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-2.10518837,  2.7672143 ,  0.04912144, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.65430808, -0.50565737,  3.08325768, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-1.91853213, -0.04695213,  3.0546906 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.88881642,  2.54712439, -2.18098855, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-2.09290457, -0.86367798,  4.3063097 , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distortions_euclidean = []\n",
    "# distortions_minkowski = []\n",
    "# distortions_cityblock = []\n",
    "# inertias = []\n",
    "# mapping1 = {}\n",
    "# mapping2 = {}\n",
    "# K = range(1, 10)\n",
    "\n",
    "# for k in K:\n",
    "#     # Building and fitting the model\n",
    "#     kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "#     kmeanModel.fit(X)\n",
    "\n",
    "#     distortions_euclidean.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
    "#                                         'euclidean'), axis=1)) / X.shape[0])\n",
    "#     distortions_minkowski.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
    "#                                         'minkowski'), axis=1)) / X.shape[0])\n",
    "#     distortions_cityblock.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
    "#                                         'cityblock'), axis=1)) / X.shape[0])\n",
    "#     inertias.append(kmeanModel.inertia_)\n",
    "\n",
    "#     mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
    "#                                    'euclidean'), axis=1)) / X.shape[0]\n",
    "#     mapping2[k] = kmeanModel.inertia_\n",
    "    \n",
    "#     print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, val in mapping1.items():\n",
    "# #     print(f'{key} : {val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = range(1,10)\n",
    "# plt.plot(K, distortions_euclidean, 'bx-')\n",
    "# plt.plot(K, distortions_minkowski, 'gx-')\n",
    "# plt.plot(K, distortions_cityblock, 'rx-')\n",
    "# plt.xlabel('Values of K')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.title('The Elbow Method using Distortion')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, val in mapping2.items():\n",
    "#    print(f'{key} : {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(K, inertias, 'bx-')\n",
    "# plt.xlabel('Values of K')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('The Elbow Method using Inertia')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6748, 3,\n",
       "       <bound method _cs_matrix.toarray of <6748x25569 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 102491 stored elements in Compressed Sparse Row format>>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=10\n",
    "kmeans = KMeans(n_clusters=n, random_state=0).fit(X)\n",
    "kmeans.labels_\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_label'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['neun_euro_context'] = df['neun_euro_context']\n",
    "df_auswertung = df.groupby('kmeans_label').mean()\n",
    "df_auswertung['cluster_size'] =  np.bincount(kmeans.labels_)\n",
    "df_auswertung['cluster_label'] =  range(10)\n",
    "df_auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_auswertung.plot.bar(x='cluster_label', y='cluster_size', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_auswertung[['positive', 'negative', 'neutral']].plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-easily-cluster-textual-data-in-python-ab27040b07d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['kmeans_label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags = pd.DataFrame()\n",
    "for k in range(10):\n",
    "    df_tmp = pd.DataFrame()\n",
    "    df_tmp_2 = pd.DataFrame()\n",
    "    df_tmp['hastag_list'] = df[df.kmeans_label == k].text.str.findall(r'#.*?(?=\\s|$)')\n",
    "    df_tmp['number_hastags'] = df_tmp['hastag_list'].apply(lambda x: len(x))\n",
    "    df_tmp = df_tmp[df_tmp.number_hastags != 3]\n",
    "    hashtags = list()\n",
    "    for i, row in df_tmp.iterrows():\n",
    "        hashtags += row['hastag_list']\n",
    "    hashtag_dict = Counter([s.translate(str.maketrans('', '', string.punctuation)) for s in hashtags])\n",
    "    tmp_dict = {k: v for k, v in sorted(hashtag_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    df_tmp_2 = df_tmp_2.from_dict(tmp_dict, orient='index', dtype=None, columns=None).reset_index()\n",
    "    df_tmp_2['cluster'] = k\n",
    "    df_hashtags = pd.concat([df_hashtags,df_tmp_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags.groupby('cluster').head(3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['kmeans_label','neun_euro_context', 'positive', 'negative', 'neutral', 'text']].to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.neun_euro_context][['positive', 'negative', 'neutral']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.neun_euro_context][['positive', 'negative', 'neutral']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgba\n",
    "sns.scatterplot(data=df[df.kmeans_label.isin([2,5])], x=\"negative\", y=\"neutral\", hue=\"kmeans_label\", alpha=0.4)\n",
    "# , style=\"time\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=df[df.kmeans_label.isin([i])], x=\"negative\", y=\"neutral\", hue=\"kmeans_label\", alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('kmeans_label').mean().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_a = X[list(df[df.kmeans_label==9].index)].sum(axis=0)\n",
    "nd = np.argpartition(sum_a, -10)[-10:]\n",
    "for i in nd:\n",
    "    print(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.stop_words_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
